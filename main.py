import uuid
import os
import shutil
from fastapi import FastAPI, UploadFile, File, Form, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from typing import Optional
import boto3
from botocore.exceptions import ClientError
from PIL import Image
import numpy as np
import PyOpenColorIO as ocio
import cv2
from moviepy.editor import VideoFileClip
from datetime import datetime, timedelta

# ------------------
# Configuration
# ------------------
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# S3 configuration
S3_BUCKET = os.getenv("S3_BUCKET", "ocio-outputs")
S3_REGION = os.getenv("AWS_REGION", "us-east-2")
PRESIGNED_URL_EXPIRY = int(os.getenv("PRESIGNED_URL_EXPIRY", "3600"))  # 1 hour

# Initialize S3 client
s3 = boto3.client('s3', region_name=S3_REGION)

# Temp directory for job files
TEMP_DIR = "/tmp/ocio_jobs"
os.makedirs(TEMP_DIR, exist_ok=True)

# OCIO Configuration
OCIO_CONFIG_PATH = os.getenv("OCIO", "/opt/ocio/configs/aces_1.3/config.ocio")

# Load OCIO config
try:
    if os.path.exists(OCIO_CONFIG_PATH):
        config = ocio.Config.CreateFromFile(OCIO_CONFIG_PATH)
        ocio.SetCurrentConfig(config)
        print(f"Loaded OCIO config from: {OCIO_CONFIG_PATH}")
    else:
        config = ocio.GetCurrentConfig()
        print(f"Warning: Using default OCIO config")
except Exception as e:
    print(f"Error loading OCIO config: {e}")
    config = ocio.Config.CreateRaw()

# In-memory job store
jobs = {}

# ------------------
# Utility Functions
# ------------------

def upload_to_s3(local_path, s3_key):
    """Upload file to S3 and return the S3 key (not URL)"""
    try:
        s3.upload_file(local_path, S3_BUCKET, s3_key)
        print(f"Uploaded {local_path} to s3://{S3_BUCKET}/{s3_key}")
        return s3_key
    except ClientError as e:
        print(f"Error uploading to S3: {e}")
        raise

def generate_presigned_url(s3_key, expiration=3600):
    """Generate a presigned URL for private S3 object"""
    try:
        response = s3.generate_presigned_url(
            'get_object',
            Params={
                'Bucket': S3_BUCKET,
                'Key': s3_key
            },
            ExpiresIn=expiration
        )
        print(f"Generated presigned URL for {s3_key}, expires in {expiration} seconds")
        return response
    except ClientError as e:
        print(f"Error generating presigned URL: {e}")
        return None

def extract_frame_from_video(video_path, frame_time=0):
    """Extract a frame from video for preview"""
    clip = VideoFileClip(video_path)
    frame = clip.get_frame(frame_time)
    clip.close()
    
    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    pil_image = Image.fromarray(frame_rgb)
    return pil_image

def is_video_file(filename):
    """Check if file is a video based on extension"""
    video_extensions = ['.mp4', '.mov', '.avi', '.mkv', '.webm', '.m4v', '.wmv', '.flv']
    return any(filename.lower().endswith(ext) for ext in video_extensions)

def process_image(file_path, input_cs, output_cs):
    """Process image or video frame using OCIO"""
    img = Image.open(file_path).convert("RGB")
    arr = np.array(img).astype(np.float32) / 255.0
    
    config = ocio.GetCurrentConfig()
    
    # Validate color spaces exist
    available_spaces = [cs.getName() for cs in config.getColorSpaces()]
    if input_cs not in available_spaces:
        raise ValueError(f"Input color space '{input_cs}' not found. Available: {available_spaces}")
    if output_cs not in available_spaces:
        raise ValueError(f"Output color space '{output_cs}' not found. Available: {available_spaces}")
    
    processor = config.getProcessor(input_cs, output_cs)
    
    shape = arr.shape
    flat = arr.reshape(-1, 3)
    flat = processor.applyRGB(flat)
    arr = flat.reshape(shape)
    arr = np.clip(arr * 255, 0, 255).astype(np.uint8)
    
    if is_video_file(file_path):
        preview_path = file_path.rsplit('.', 1)[0] + "_preview.png"
    else:
        preview_path = file_path.replace(".", "_preview.")
    
    Image.fromarray(arr).save(preview_path)
    return preview_path

def generate_lut(job_id, input_cs, output_cs, lut_size=64):
    """Generate a LUT for the color space conversion"""
    config = ocio.GetCurrentConfig()
    processor = config.getProcessor(input_cs, output_cs)
    
    lut_path = os.path.join(TEMP_DIR, f"{job_id}.cube")
    
    with open(lut_path, "w") as f:
        f.write(f"# LUT generated by OCIO Service\n")
        f.write(f"# Input: {input_cs} -> Output: {output_cs}\n")
        f.write(f"LUT_3D_SIZE {lut_size}\n")
        
        for b in range(lut_size):
            for g in range(lut_size):
                for r in range(lut_size):
                    rgb = np.array([
                        r / (lut_size - 1),
                        g / (lut_size - 1), 
                        b / (lut_size - 1)
                    ], dtype=np.float32)
                    
                    transformed = processor.applyRGB(rgb)
                    f.write(f"{transformed[0]:.6f} {transformed[1]:.6f} {transformed[2]:.6f}\n")
    
    return lut_path

# ------------------
# Async Job Processor
# ------------------

def process_job(job_id: str, file_path: str, input_cs: str, output_cs: str, look: Optional[str], lut_size: Optional[int]):
    try:
        jobs[job_id]["status"] = "running"
        
        is_video = is_video_file(file_path)
        
        # Process file and get preview
        if is_video:
            frame = extract_frame_from_video(file_path)
            frame_path = os.path.join(TEMP_DIR, f"{job_id}_frame.png")
            frame.save(frame_path)
            preview_path = process_image(frame_path, input_cs, output_cs)
            os.remove(frame_path)
        else:
            preview_path = process_image(file_path, input_cs, output_cs)
        
        # Generate LUT
        lut_size = lut_size or 64
        lut_path = generate_lut(job_id, input_cs, output_cs, lut_size)
        
        # Upload files to S3 and get keys
        preview_key = f"jobs/{job_id}/preview.png"
        lut_key = f"jobs/{job_id}/lut.cube"
        
        upload_to_s3(preview_path, preview_key)
        upload_to_s3(lut_path, lut_key)
        
        # Generate presigned URLs (valid for 1 hour)
        preview_url = generate_presigned_url(preview_key, PRESIGNED_URL_EXPIRY)
        lut_url = generate_presigned_url(lut_key, PRESIGNED_URL_EXPIRY)
        
        # Update job status with URLs that will expire
        jobs[job_id]["status"] = "completed"
        jobs[job_id]["preview_url"] = preview_url
        jobs[job_id]["lut_url"] = lut_url
        jobs[job_id]["is_video"] = is_video
        jobs[job_id]["urls_expire_at"] = (datetime.now() + timedelta(seconds=PRESIGNED_URL_EXPIRY)).isoformat()
        
        # Cleanup temp files
        for path in [file_path, preview_path, lut_path]:
            if os.path.exists(path):
                os.remove(path)
                
    except Exception as e:
        jobs[job_id]["status"] = "failed"
        jobs[job_id]["error"] = str(e)
        print(f"Job {job_id} failed: {e}")

# ------------------
# Endpoints
# ------------------

@app.get("/health")
def health():
    return {"status": "OK"}

@app.get("/colorspaces")
def colorspaces():
    config = ocio.GetCurrentConfig()
    spaces = [cs.getName() for cs in config.getColorSpaces()]
    return {"spaces": spaces}

@app.post("/jobs")
async def create_job(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    input_colorspace: str = Form(...),
    output_colorspace: str = Form(...),
    look: Optional[str] = Form(None),
    lut_size: Optional[int] = Form(64)
):
    job_id = str(uuid.uuid4())
    ext = os.path.splitext(file.filename)[1]
    file_path = os.path.join(TEMP_DIR, f"{job_id}{ext}")
    
    # Save uploaded file
    with open(file_path, "wb") as f:
        content = await file.read()
        f.write(content)
    
    # Validate color spaces
    config = ocio.GetCurrentConfig()
    available_spaces = [cs.getName() for cs in config.getColorSpaces()]
    
    if input_colorspace not in available_spaces:
        return {"error": f"Input color space '{input_colorspace}' not found"}
    if output_colorspace not in available_spaces:
        return {"error": f"Output color space '{output_colorspace}' not found"}
    
    # Register job
    jobs[job_id] = {
        "status": "queued",
        "preview_url": None,
        "lut_url": None,
        "is_video": is_video_file(file.filename),
        "urls_expire_at": None
    }
    
    # Launch async processing
    background_tasks.add_task(
        process_job,
        job_id,
        file_path,
        input_colorspace,
        output_colorspace,
        look,
        lut_size
    )
    
    return {
        "job_id": job_id,
        "status": "queued",
        "is_video": is_video_file(file.filename),
        "message": "Processing started. Check /jobs/{job_id} for status."
    }

@app.get("/jobs/{job_id}")
def get_job_status(job_id: str):
    job = jobs.get(job_id)
    if not job:
        return {"status": "not_found"}
    
    response = {
        "status": job["status"],
        "job_id": job_id,
        "is_video": job.get("is_video", False),
        "urls_expire_at": job.get("urls_expire_at"),
        "outputs": {}
    }
    
    if job["status"] == "completed":
        # Regenerate presigned URLs if they're about to expire
        if job.get("urls_expire_at"):
            expiry_time = datetime.fromisoformat(job["urls_expire_at"])
            if datetime.now() > expiry_time - timedelta(minutes=5):
                # URLs expiring soon, generate new ones
                preview_key = f"jobs/{job_id}/preview.png"
                lut_key = f"jobs/{job_id}/lut.cube"
                
                job["preview_url"] = generate_presigned_url(preview_key, PRESIGNED_URL_EXPIRY)
                job["lut_url"] = generate_presigned_url(lut_key, PRESIGNED_URL_EXPIRY)
                job["urls_expire_at"] = (datetime.now() + timedelta(seconds=PRESIGNED_URL_EXPIRY)).isoformat()
        
        response["outputs"]["preview_url"] = job.get("preview_url")
        response["outputs"]["lut_url"] = job.get("lut_url")
    
    elif job["status"] == "failed":
        response["error"] = job.get("error")
    
    return response

@app.get("/jobs/{job_id}/refresh-urls")
def refresh_urls(job_id: str):
    """Refresh presigned URLs for a completed job"""
    job = jobs.get(job_id)
    if not job or job["status"] != "completed":
        return {"error": "Job not found or not completed"}
    
    preview_key = f"jobs/{job_id}/preview.png"
    lut_key = f"jobs/{job_id}/lut.cube"
    
    try:
        job["preview_url"] = generate_presigned_url(preview_key, PRESIGNED_URL_EXPIRY)
        job["lut_url"] = generate_presigned_url(lut_key, PRESIGNED_URL_EXPIRY)
        job["urls_expire_at"] = (datetime.now() + timedelta(seconds=PRESIGNED_URL_EXPIRY)).isoformat()
        
        return {
            "status": "success",
            "message": "URLs refreshed",
            "urls_expire_at": job["urls_expire_at"]
        }
    except Exception as e:
        return {"error": f"Failed to refresh URLs: {str(e)}"}